import os
import time
import random
from datetime import datetime

import feedparser
import trafilatura
import google.generativeai as genai
from dotenv import load_dotenv

from db import get_connection
from sources import RSS_SOURCES
# -------------------------------
# ‚öôÔ∏è CONFIG
# -------------------------------
load_dotenv()

GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY")
MODEL = "gemini-2.5-flash"

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel(MODEL)

# -------------------------------
# üìÖ –ë–µ–∑–ø–µ—á–Ω–∏–π –ø–∞—Ä—Å–µ—Ä –¥–∞—Ç–∏
# -------------------------------
def parse_pubdate(pubdate_str):
    if not pubdate_str:
        return None
    for fmt in ("%a, %d %b %Y %H:%M:%S %z", "%a, %d %b %Y %H:%M:%S %Z"):
        try:
            return datetime.strptime(pubdate_str, fmt)
        except ValueError:
            continue
    return None

# -------------------------------
# üì∞ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è RSS
# -------------------------------
def fetch_rss_entries(source_name, rss_url, limit=5):
    print(f"üîç Henter nyheder fra {source_name}...")
    feed = feedparser.parse(rss_url)
    for entry in feed.entries[:limit]:
        yield {
            "title": entry.title,
            "link": entry.link,
            "published": getattr(entry, "published", None),
            "source": source_name,
        }

# -------------------------------
# üìú –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
# -------------------------------
def get_full_text(url):
    downloaded = trafilatura.fetch_url(url)
    if downloaded:
        return trafilatura.extract(downloaded)
    return None

# -------------------------------
# ü§ñ –ü—ñ–¥—Å—É–º–æ–∫ —á–µ—Ä–µ–∑ Gemini
# -------------------------------
def summarize_text_danish(text):
    prompt = (
        "Lav et kort nyhedsresum√© p√• dansk i 2-3 s√¶tninger. "
        "Behold fakta og skriv i neutral journalistisk stil:\n\n"
        f"{text}"
    )
    response = model.generate_content(prompt)
    return response.text.strip() if response and response.text else "Ingen resum√©."

# -------------------------------
# üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —É –±–∞–∑—É –¥–∞–Ω–∏—Ö
# -------------------------------
def save_to_db(news_list):
    if not news_list:
        return
    conn = get_connection()
    cur = conn.cursor()

    for news in news_list:
        cur.execute("SELECT id FROM news WHERE link = %s", (news["link"],))
        if cur.fetchone():
            continue  # —É–Ω–∏–∫–∞—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤

        cur.execute("""
            INSERT INTO news (title, link, pubDate, source, shortText)
            VALUES (%s, %s, %s, %s, %s)
        """, (
            news["title"],
            news["link"],
            news.get("pubDate"),
            news["source"],
            news["summary"],
        ))

    conn.commit()
    cur.close()
    conn.close()
    print(f"üóÑÔ∏è {len(news_list)} news items saved to DB.")

# -------------------------------
# üîÅ –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –¥–∂–µ—Ä–µ–ª
# -------------------------------
def fetch_all_news(limit=5):
    all_news = []
    for source_name, rss_url in RSS_SOURCES.items():
        for entry in fetch_rss_entries(source_name, rss_url, limit):
            full_text = get_full_text(entry["link"])
            if not full_text:
                continue

            try:
                summary = summarize_text_danish(full_text)
            except Exception as e:
                print(f"‚ö†Ô∏è AI fejl for '{entry['title']}': {e}")
                summary = "Kunne ikke generere resum√©."

            pub_date = parse_pubdate(entry.get("published"))

            news_item = {
                "title": entry["title"],
                "link": entry["link"],
                "pubDate": pub_date,
                "source": entry["source"],
                "summary": summary,
            }
            all_news.append(news_item)
            time.sleep(random.uniform(0.3, 0.8))
    return all_news

# -------------------------------
# üß† –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è
# -------------------------------
def update_news_cache():
    try:
        new_data = fetch_all_news(limit=3)
        save_to_db(new_data)
        print(f"‚úÖ News fetched and saved ({len(new_data)} items).")
    except Exception as e:
        print(f"‚ùå Error updating news cache: {e}")
