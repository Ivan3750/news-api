# --- Fix for feedparser on Python 3.13 (cgi removed) ---
import sys, types
if "cgi" not in sys.modules:
    cgi = types.ModuleType("cgi")
    cgi.parse_header = lambda value: (value, {})
    sys.modules["cgi"] = cgi
# -------------------------------------------------------

import feedparser
import os
import time
import random
from datetime import datetime
import trafilatura
import google.generativeai as genai
from dotenv import load_dotenv
from db import get_connection
from sources import RSS_SOURCES

# -------------------------------
# ‚öôÔ∏è CONFIG
# -------------------------------
load_dotenv()

# –¥–≤–∞ –∫–ª—é—á—ñ
GEMINI_KEYS = [
    os.getenv("GOOGLE_API_KEY_MAIN"),
    os.getenv("GOOGLE_API_KEY_BACKUP"),
]
MODEL = "gemini-2.5-flash"

active_key_index = 0  # –ø–æ—Ç–æ—á–Ω–∏–π –∫–ª—é—á
genai.configure(api_key=GEMINI_KEYS[active_key_index])
model = genai.GenerativeModel(MODEL)


# -------------------------------
# üîÑ –ü–µ—Ä–µ–º–∏–∫–∞–Ω–Ω—è –∫–ª—é—á–∞
# -------------------------------
def switch_key():
    global active_key_index, model
    active_key_index = 1 - active_key_index
    new_key = GEMINI_KEYS[active_key_index]
    genai.configure(api_key=new_key)
    model = genai.GenerativeModel(MODEL)
    print(f"üîë Switched to API key {active_key_index + 1}")


# -------------------------------
# üìÖ –ë–µ–∑–ø–µ—á–Ω–∏–π –ø–∞—Ä—Å–µ—Ä –¥–∞—Ç–∏
# -------------------------------
def parse_pubdate(pubdate_str):
    if not pubdate_str:
        return None
    for fmt in ("%a, %d %b %Y %H:%M:%S %z", "%a, %d %b %Y %H:%M:%S %Z"):
        try:
            return datetime.strptime(pubdate_str, fmt)
        except ValueError:
            continue
    return None


# -------------------------------
# üì∞ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è RSS
# -------------------------------
def fetch_rss_entries(source_name, rss_url, limit=5):
    print(f"üîç Henter nyheder fra {source_name}...")
    feed = feedparser.parse(rss_url)
    for entry in feed.entries[:limit]:
        yield {
            "title": entry.title,
            "link": entry.link,
            "published": getattr(entry, "published", None),
            "source": source_name,
        }


# -------------------------------
# üìú –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
# -------------------------------
def get_full_text(url):
    downloaded = trafilatura.fetch_url(url)
    if downloaded:
        return trafilatura.extract(downloaded)
    return None


# -------------------------------
# ü§ñ –ü—ñ–¥—Å—É–º–æ–∫ —á–µ—Ä–µ–∑ Gemini (–∑ fallback)
# -------------------------------
def summarize_text_danish(text):
    global model
    prompt = (
        "Lav et kort nyhedsresum√© p√• dansk i 2-3 s√¶tninger. "
        "Behold fakta og skriv i neutral journalistisk stil:\n\n"
        f"{text}"
    )

    for attempt in range(2):  # –º–∞–∫—Å–∏–º—É–º 2 —Å–ø—Ä–æ–±–∏ (–ø–æ –æ–¥–Ω–æ–º—É –∫–ª—é—á—É)
        try:
            response = model.generate_content(prompt)
            if response and response.text:
                return response.text.strip()
            else:
                raise ValueError("Empty response from Gemini")
        except Exception as e:
            print(f"‚ö†Ô∏è AI fejl (attempt {attempt+1}) med key {active_key_index+1}: {e}")
            if attempt == 0:
                # –ø—Ä–æ–±—É—î–º–æ —ñ–Ω—à–∏–π –∫–ª—é—á
                switch_key()
                time.sleep(1)
            else:
                print("‚ùå Both keys failed ‚Äî skipping this news item.")
                return "Kunne ikke generere resum√©."
    return "Kunne ikke generere resum√©."


# -------------------------------
# üè∑Ô∏è –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
# -------------------------------
def classify_category_danish(text):
    global model
    categories = ["Alle", "Politik", "√òkonomi", "Sport", "Milj√∏", "Teknologi"]

    prompt = (
        "L√¶s denne danske nyhedsartikel og bestem, hvilken kategori den tilh√∏rer. "
        "V√¶lg KUN √©n af f√∏lgende kategorier:\n\n"
        "Alle, Politik, √òkonomi, Sport, Milj√∏, Teknologi.\n\n"
        "Svar KUN med navnet p√• kategorien uden forklaring.\n\n"
        f"Artikel:\n{text}"
    )

    for attempt in range(2):
        try:
            response = model.generate_content(prompt)
            if not response or not response.text:
                raise ValueError("Empty response")

            cat = response.text.strip()
            # –æ—á–∏—Å—Ç–∏–º–æ –≤—ñ–¥ –ª–∞–ø–æ–∫ —ñ –ø—Ä–æ–±—ñ–ª—ñ–≤
            cat = cat.replace('"', '').replace("'", "").strip()

            # –ø–µ—Ä–µ–≤—ñ—Ä–∏–º–æ, —á–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è –≤–∞–ª—ñ–¥–Ω–∞
            for c in categories:
                if c.lower() in cat.lower():
                    return c

            # —è–∫—â–æ –Ω–µ –≤–ø—ñ–∑–Ω–∞–ª–∏
            return "Alle"

        except Exception as e:
            print(f"‚ö†Ô∏è Category AI fejl (attempt {attempt+1}) med key {active_key_index+1}: {e}")
            if attempt == 0:
                switch_key()
                time.sleep(1)
            else:
                print("‚ùå Both keys failed for category ‚Äî fallback to 'Alle'.")
                return "Alle"

    return "Alle"


# -------------------------------
# üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —É –±–∞–∑—É –¥–∞–Ω–∏—Ö
# -------------------------------
def save_to_db(news_list):
    if not news_list:
        return
    conn = get_connection()
    cur = conn.cursor()

    for news in news_list:
        cur.execute("SELECT id FROM news WHERE link = %s", (news["link"],))
        if cur.fetchone():
            continue  # —É–Ω–∏–∫–∞—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤

        cur.execute("""
            INSERT INTO news (title, link, pubDate, source, shortText, category)
            VALUES (%s, %s, %s, %s, %s, %s)
        """, (
            news["title"],
            news["link"],
            news.get("pubDate"),
            news["source"],
            news["summary"],
            news["category"],
        ))

    conn.commit()
    cur.close()
    conn.close()
    print(f"üóÑÔ∏è {len(news_list)} news items saved to DB.")


# -------------------------------
# üîÅ –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –¥–∂–µ—Ä–µ–ª
# -------------------------------
def fetch_all_news(limit=5):
    all_news = []
    for source_name, rss_url in RSS_SOURCES.items():
        for entry in fetch_rss_entries(source_name, rss_url, limit):
            full_text = get_full_text(entry["link"])
            if not full_text:
                continue

            summary = summarize_text_danish(full_text)
            category = classify_category_danish(summary)

            pub_date = parse_pubdate(entry.get("published"))
            news_item = {
                "title": entry["title"],
                "link": entry["link"],
                "pubDate": pub_date,
                "source": entry["source"],
                "summary": summary,
                "category": category,
            }
            all_news.append(news_item)
            time.sleep(random.uniform(0.6, 1.3))  # —Ç—Ä–æ—Ö–∏ –±—ñ–ª—å—à–∞ –ø–∞—É–∑–∞, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ª—ñ–º—ñ—Ç—ñ–≤
    return all_news


# -------------------------------
# üß† –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è
# -------------------------------
def update_news_cache():
    try:
        new_data = fetch_all_news(limit=3)
        save_to_db(new_data)
        print(f"‚úÖ News fetched and saved ({len(new_data)} items).")
    except Exception as e:
        print(f"‚ùå Error updating news cache: {e}")
